# Konkani Tokenizer

A custom tokenizer built using SentencePiece for Automatic Speech Recognition (ASR) fine-tuning in the Konkani language. This tokenizer is trained on a curated dataset consisting of Konkani textbooks and transcribed audio, optimized for use with LLMs and speech models.

## ðŸš€ Features

- SentencePiece-based tokenizer
- Designed specifically for Konkani language
- Compatible with ASR fine-tuning workflows
- Trained on real-world textbook and audio transcript data

## ðŸ›  Installation & Usage

```bash
pip install git+https://github.com/Banashankar-Tatalagera/konkani_tokenizerV02.git
```


ðŸ“¦ Dependencies

    tokenizers

    sentencepiece






## for testing the tokenizer
### save this code in the jupyterNOtebook or .py file and run:

```bash
from konkani_tokenizer import KonkaniTokenizer

tokenizer = KonkaniTokenizer()

text = "à¤¹à¤¾à¤‚à¤µ à¤•à¤¾à¤²à¥‡à¤° à¤®à¥‹à¤°à¤—à¤¾à¤‚à¤µ à¤—à¥‡à¤²à¥‹ à¤†à¤‚à¤µ à¤­à¤¿à¤¤à¤° à¤¥à¥‹à¤¡à¥‡ à¤µà¥‡à¤³à¤¾à¤šà¥‡à¤° à¤®à¤¿à¤¤à¤°à¤¾à¤‚à¤• à¤¸à¥‹à¤¬à¤¤ à¤‰à¤¬à¥‹à¤‚à¤Ÿ à¤†à¤¨à¥€ à¤šà¤¾à¤¯ à¤˜à¥‡à¤¤à¤²à¥‹, à¤¤à¤¾à¤‚à¤šà¥‡ à¤¸à¤‚à¤—à¤¤ à¤®à¤œà¥‡à¤šà¥‡à¤° à¤†à¤¸à¤¾."
tokens = tokenizer.tokenize(text)
ids = tokenizer.encode(text)
decoded = tokenizer.decode(ids)

print("Tokens:", tokens)
print("Token IDs:", ids)
print("Decoded Text:", decoded)

```